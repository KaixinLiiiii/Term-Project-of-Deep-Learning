{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGG16.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPqsGHPmStbJnm2uWN6/+Yc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"9Cg6z1KDrkXf"},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","\n","!apt-get update -qq 2>&1 > /dev/null\n","\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","\n","from google.colab import auth\n","\n","auth.authenticate_user()\n","\n","from oauth2client.client import GoogleCredentials\n","\n","creds = GoogleCredentials.get_application_default()\n","\n","import getpass\n","\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","\n","vcode = getpass.getpass()\n","\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AmNjfC-yrnCa"},"source":["!mkdir -p drive \n","\n","!google-drive-ocamlfuse drive "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tWABeDIsrnuu"},"source":["!pwd\n","!ls drive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hjpxc40grpSa"},"source":["!pip3 install torch torchvision"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H3VThwuteDvN"},"source":["import torch\n","import math\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","import time\n","import os\n","import numpy as np\n","import torch.backends.cudnn as cudnn\n","\n","\n","class Cutout(object):\n","    \"\"\"Randomly mask out one or more patches from an image.\n","    Args:\n","        n_holes (int): Number of patches to cut out of each image.\n","        length (int): The length (in pixels) of each square patch.\n","    \"\"\"\n","    def __init__(self, n_holes, length):\n","        self.n_holes = n_holes\n","        self.length = length\n","\n","    def __call__(self, img):\n","        \"\"\"\n","        Args:\n","            img (Tensor): Tensor image of size (C, H, W).\n","        Returns:\n","            Tensor: Image with n_holes of dimension length x length cut out of it.\n","        \"\"\"\n","        h = img.size(1)\n","        w = img.size(2)\n","\n","        mask = np.ones((h, w), np.float32)\n","\n","        for n in range(self.n_holes):\n","            y = np.random.randint(h)\n","            x = np.random.randint(w)\n","\n","            y1 = np.clip(y - self.length // 2, 0, h)\n","            y2 = np.clip(y + self.length // 2, 0, h)\n","            x1 = np.clip(x - self.length // 2, 0, w)\n","            x2 = np.clip(x + self.length // 2, 0, w)\n","\n","            mask[y1: y2, x1: x2] = 0.\n","\n","        mask = torch.from_numpy(mask)\n","        mask = mask.expand_as(img)\n","        img = img * mask\n","\n","        return img\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n","start_time = time.time()\n","batch_size = 128\n","learning_rate = 0.001\n","\n","lr_decay_epochs = [60,80,100]\n","lr_decay_rate = 0.1\n","\n","root_dir = 'drive/app/cifar10/'\n","default_directory = 'drive/app/torch/save_models'\n","\n","# Data Augmentation\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n","    transforms.RandomHorizontalFlip(),                  # right and left flip\n","    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n","    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n","                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n","])\n","transform_train.transforms.append(Cutout(n_holes=1, length=16))\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n","    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n","                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n","])\n","\n","# automatically download\n","train_dataset = datasets.CIFAR10(root=root_dir,\n","                                 train=True,\n","                                 transform=transform_train,\n","                                 download=True)\n","\n","test_dataset = datasets.CIFAR10(root=root_dir,\n","                                train=False,\n","                                transform=transform_test)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n","                                           num_workers=4)           # CPU loader number\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n","                                          num_workers=4)            # CPU loader number\n","\n","\n","class VGG16(nn.Module):\n","\n","    def __init__(self, num_classes=10):\n","        super(VGG16, self).__init__()\n","        self.features = nn.Sequential(\n","            \n","            # Frist part of conv\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Second part of conv\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Third part of conv\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Fourth part of conv\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ELU(alpha=1.0, inplace=False),\n","            \n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Fifth part of conv\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ELU(alpha=1.0, inplace=False),\n","            \n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512*1*1, 256),\n","            nn.ELU(),\n","            nn.Dropout(),\n","            nn.Linear(256, 256),\n","            nn.ELU(),\n","            nn.Dropout(),\n","            nn.Linear(256, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        ex_feature = self.features(x)\n","        ex_feature = ex_feature.view(-1, 512*1*1)\n","        classify_result = self.classifier(ex_feature)\n","        return classify_result\n","\n","model = VGG16()\n","\n","optimizer = optim.SGD(model.parameters(), learning_rate,\n","                                momentum=0.9,\n","                                weight_decay=1e-4,\n","                                nesterov=True)\n","criterion = nn.CrossEntropyLoss()\n","\n","if torch.cuda.device_count() > 0:\n","    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n","    model = nn.DataParallel(model).cuda()\n","    cudnn.benchmark = True\n","else:\n","    print(\"USE ONLY CPU!\")\n","\n","\n","def train(epoch):\n","    model.train()\n","    train_loss = 0 \n","    total = 0\n","    correct = 0\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        if torch.cuda.is_available():\n","            data, target = Variable(data.cuda()), Variable(target.cuda())\n","        else:\n","            data, target = Variable(data), Variable(target)\n","\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = torch.max(output.data, 1)\n","\n","        total += target.size(0)\n","        correct += predicted.eq(target.data).cpu().sum()\n","        if batch_idx % 10 == 0:\n","            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n","                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n","\n","\n","def test():\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (data, target) in enumerate(test_loader):\n","        if torch.cuda.is_available():\n","            data, target = Variable(data.cuda()), Variable(target.cuda())\n","        else:\n","            data, target = Variable(data), Variable(target)\n","\n","        outputs = model(data)\n","        loss = criterion(outputs, target)\n","\n","        test_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += target.size(0)\n","        correct += predicted.eq(target.data).cpu().sum()\n","    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n","          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n","\n","\n","def save_checkpoint(directory, state, filename='latest.tar.gz'):\n","\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","    model_filename = os.path.join(directory, filename)\n","    torch.save(state, model_filename)\n","    print(\"=> saving checkpoint\")\n","\n","def load_checkpoint(directory, filename='latest.tar.gz'):\n","\n","    model_filename = os.path.join(directory, filename)\n","    if os.path.exists(model_filename):\n","        print(\"=> loading checkpoint\")\n","        state = torch.load(model_filename)\n","        return state\n","    else:\n","        return None\n","\n","start_epoch = 0\n","\n","checkpoint = load_checkpoint(default_directory)\n","if not checkpoint:\n","    pass\n","else:\n","    start_epoch = checkpoint['epoch'] + 1\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","\n","def adjust_learning_rate(epoch, lr_decay_epochs, lr_decay_rate, learning_rate, optimizer):\n","    \"\"\"Sets the learning rate to the initial LR decayed by 0.2 every steep step\"\"\"\n","    steps = np.sum(epoch > np.asarray(lr_decay_epochs))\n","    if steps > 0:\n","        new_lr = learning_rate * (lr_decay_rate ** steps)\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = new_lr\n","\n","for epoch in range(start_epoch, 150):\n","\n","    adjust_learning_rate(epoch, lr_decay_epochs, lr_decay_rate, learning_rate, optimizer)\n","\n","    train(epoch)\n","    save_checkpoint(default_directory, {\n","        'epoch': epoch,\n","        'model': model,\n","        'state_dict': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","    })\n","    test()  \n","\n","now = time.gmtime(time.time() - start_time)\n","print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rIDLBNJxXucT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"5tBMSV0SWCFp"},"execution_count":null,"outputs":[]}]}