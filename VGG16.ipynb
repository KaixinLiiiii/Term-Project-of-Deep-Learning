{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGG16.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPqsGHPmStbJnm2uWN6/+Yc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Cg6z1KDrkXf","executionInfo":{"status":"ok","timestamp":1639290501625,"user_tz":-540,"elapsed":55591,"user":{"displayName":"Kaixin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18137885756968183259"}},"outputId":"a676fc55-bcb5-4612-9770-6c66461f999e"},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","\n","!apt-get update -qq 2>&1 > /dev/null\n","\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","\n","from google.colab import auth\n","\n","auth.authenticate_user()\n","\n","from oauth2client.client import GoogleCredentials\n","\n","creds = GoogleCredentials.get_application_default()\n","\n","import getpass\n","\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","\n","vcode = getpass.getpass()\n","\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 155222 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.27-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"]}]},{"cell_type":"code","metadata":{"id":"AmNjfC-yrnCa"},"source":["!mkdir -p drive \n","\n","!google-drive-ocamlfuse drive "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tWABeDIsrnuu","executionInfo":{"status":"ok","timestamp":1639290502514,"user_tz":-540,"elapsed":897,"user":{"displayName":"Kaixin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18137885756968183259"}},"outputId":"a19b8668-7e38-49ae-bfb9-db670ea583b1"},"source":["!pwd\n","!ls drive"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"," 7A.cmbl\t\t    HW_1_AnswerKey-1.pdf   RIS\n"," app\t\t\t    HW1.ipynb\t\t   Term-Project.ipynb（副本）\n","'Colab Notebooks'\t    HW5.ipynb\t\t   未命名文件夹\n","'DL-Term Project'\t    MAH02275.MP4\n"," HW_1_AnswerKey-1.desktop   MAH02278.MP4\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hjpxc40grpSa","executionInfo":{"status":"ok","timestamp":1639290506421,"user_tz":-540,"elapsed":3913,"user":{"displayName":"Kaixin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18137885756968183259"}},"outputId":"0a89b5c9-b8e6-42d4-ddb5-d5993c14dd50"},"source":["!pip3 install torch torchvision"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":777},"id":"H3VThwuteDvN","executionInfo":{"status":"error","timestamp":1639379025448,"user_tz":-540,"elapsed":38298,"user":{"displayName":"Kaixin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18137885756968183259"}},"outputId":"e0ec0f89-d2c6-48df-d358-65806a6aa99a"},"source":["import torch\n","import math\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms\n","import torch.nn.functional as F\n","import time\n","import os\n","import numpy as np\n","import torch.backends.cudnn as cudnn\n","\n","\n","class Cutout(object):\n","    \"\"\"Randomly mask out one or more patches from an image.\n","    Args:\n","        n_holes (int): Number of patches to cut out of each image.\n","        length (int): The length (in pixels) of each square patch.\n","    \"\"\"\n","    def __init__(self, n_holes, length):\n","        self.n_holes = n_holes\n","        self.length = length\n","\n","    def __call__(self, img):\n","        \"\"\"\n","        Args:\n","            img (Tensor): Tensor image of size (C, H, W).\n","        Returns:\n","            Tensor: Image with n_holes of dimension length x length cut out of it.\n","        \"\"\"\n","        h = img.size(1)\n","        w = img.size(2)\n","\n","        mask = np.ones((h, w), np.float32)\n","\n","        for n in range(self.n_holes):\n","            y = np.random.randint(h)\n","            x = np.random.randint(w)\n","\n","            y1 = np.clip(y - self.length // 2, 0, h)\n","            y2 = np.clip(y + self.length // 2, 0, h)\n","            x1 = np.clip(x - self.length // 2, 0, w)\n","            x2 = np.clip(x + self.length // 2, 0, w)\n","\n","            mask[y1: y2, x1: x2] = 0.\n","\n","        mask = torch.from_numpy(mask)\n","        mask = mask.expand_as(img)\n","        img = img * mask\n","\n","        return img\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'                # GPU Number \n","start_time = time.time()\n","batch_size = 128\n","learning_rate = 0.001\n","\n","lr_decay_epochs = [60,80,100]\n","lr_decay_rate = 0.1\n","\n","root_dir = 'drive/app/cifar10/'\n","default_directory = 'drive/app/torch/save_models'\n","\n","# Data Augmentation\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),               # Random Position Crop\n","    transforms.RandomHorizontalFlip(),                  # right and left flip\n","    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n","    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n","                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n","])\n","transform_train.transforms.append(Cutout(n_holes=1, length=16))\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),                              # change [0,255] Int value to [0,1] Float value\n","    transforms.Normalize(mean=(0.4914, 0.4824, 0.4467), # RGB Normalize MEAN\n","                         std=(0.2471, 0.2436, 0.2616))  # RGB Normalize Standard Deviation\n","])\n","\n","# automatically download\n","train_dataset = datasets.CIFAR10(root=root_dir,\n","                                 train=True,\n","                                 transform=transform_train,\n","                                 download=True)\n","\n","test_dataset = datasets.CIFAR10(root=root_dir,\n","                                train=False,\n","                                transform=transform_test)\n","\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size,\n","                                           shuffle=True,            # at Training Procedure, Data Shuffle = True\n","                                           num_workers=4)           # CPU loader number\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size,\n","                                          shuffle=False,            # at Test Procedure, Data Shuffle = False\n","                                          num_workers=4)            # CPU loader number\n","\n","\n","class VGG16(nn.Module):\n","\n","    def __init__(self, num_classes=10):\n","        super(VGG16, self).__init__()\n","        self.features = nn.Sequential(\n","            \n","            # Frist part of conv\n","            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Second part of conv\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Third part of conv\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Fourth part of conv\n","            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ELU(alpha=1.0, inplace=False),\n","            \n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","            # Fifth part of conv\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ELU(alpha=1.0, inplace=False),\n","\n","            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ELU(alpha=1.0, inplace=False),\n","            \n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512*1*1, 256),\n","            nn.ELU(),\n","            nn.Dropout(),\n","            nn.Linear(256, 256),\n","            nn.ELU(),\n","            nn.Dropout(),\n","            nn.Linear(256, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        ex_feature = self.features(x)\n","        ex_feature = ex_feature.view(-1, 512*1*1)\n","        classify_result = self.classifier(ex_feature)\n","        return classify_result\n","\n","model = VGG16()\n","\n","optimizer = optim.SGD(model.parameters(), learning_rate,\n","                                momentum=0.9,\n","                                weight_decay=1e-4,\n","                                nesterov=True)\n","criterion = nn.CrossEntropyLoss()\n","\n","if torch.cuda.device_count() > 0:\n","    print(\"USE\", torch.cuda.device_count(), \"GPUs!\")\n","    model = nn.DataParallel(model).cuda()\n","    cudnn.benchmark = True\n","else:\n","    print(\"USE ONLY CPU!\")\n","\n","\n","def train(epoch):\n","    model.train()\n","    train_loss = 0 \n","    total = 0\n","    correct = 0\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        if torch.cuda.is_available():\n","            data, target = Variable(data.cuda()), Variable(target.cuda())\n","        else:\n","            data, target = Variable(data), Variable(target)\n","\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = torch.max(output.data, 1)\n","\n","        total += target.size(0)\n","        correct += predicted.eq(target.data).cpu().sum()\n","        if batch_idx % 10 == 0:\n","            print('Epoch: {} | Batch_idx: {} |  Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n","                  .format(epoch, batch_idx, train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n","\n","\n","def test():\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (data, target) in enumerate(test_loader):\n","        if torch.cuda.is_available():\n","            data, target = Variable(data.cuda()), Variable(target.cuda())\n","        else:\n","            data, target = Variable(data), Variable(target)\n","\n","        outputs = model(data)\n","        loss = criterion(outputs, target)\n","\n","        test_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += target.size(0)\n","        correct += predicted.eq(target.data).cpu().sum()\n","    print('# TEST : Loss: ({:.4f}) | Acc: ({:.2f}%) ({}/{})'\n","          .format(test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n","\n","\n","def save_checkpoint(directory, state, filename='latest.tar.gz'):\n","\n","    if not os.path.exists(directory):\n","        os.makedirs(directory)\n","\n","    model_filename = os.path.join(directory, filename)\n","    torch.save(state, model_filename)\n","    print(\"=> saving checkpoint\")\n","\n","def load_checkpoint(directory, filename='latest.tar.gz'):\n","\n","    model_filename = os.path.join(directory, filename)\n","    if os.path.exists(model_filename):\n","        print(\"=> loading checkpoint\")\n","        state = torch.load(model_filename)\n","        return state\n","    else:\n","        return None\n","\n","start_epoch = 0\n","\n","checkpoint = load_checkpoint(default_directory)\n","if not checkpoint:\n","    pass\n","else:\n","    start_epoch = checkpoint['epoch'] + 1\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","\n","def adjust_learning_rate(epoch, lr_decay_epochs, lr_decay_rate, learning_rate, optimizer):\n","    \"\"\"Sets the learning rate to the initial LR decayed by 0.2 every steep step\"\"\"\n","    steps = np.sum(epoch > np.asarray(lr_decay_epochs))\n","    if steps > 0:\n","        new_lr = learning_rate * (lr_decay_rate ** steps)\n","        for param_group in optimizer.param_groups:\n","            param_group['lr'] = new_lr\n","\n","for epoch in range(start_epoch, 150):\n","\n","    adjust_learning_rate(epoch, lr_decay_epochs, lr_decay_rate, learning_rate, optimizer)\n","\n","    train(epoch)\n","    save_checkpoint(default_directory, {\n","        'epoch': epoch,\n","        'model': model,\n","        'state_dict': model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","    })\n","    test()  \n","\n","now = time.gmtime(time.time() - start_time)\n","print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["USE 1 GPUs!\n","Epoch: 0 | Batch_idx: 0 |  Loss: (2.3826) | Acc: (7.81%) (10/128)\n","Epoch: 0 | Batch_idx: 10 |  Loss: (2.3155) | Acc: (12.78%) (180/1408)\n","Epoch: 0 | Batch_idx: 20 |  Loss: (2.2494) | Acc: (16.37%) (440/2688)\n","Epoch: 0 | Batch_idx: 30 |  Loss: (2.2022) | Acc: (17.77%) (705/3968)\n","Epoch: 0 | Batch_idx: 40 |  Loss: (2.1625) | Acc: (18.83%) (988/5248)\n","Epoch: 0 | Batch_idx: 50 |  Loss: (2.1225) | Acc: (20.83%) (1360/6528)\n","Epoch: 0 | Batch_idx: 60 |  Loss: (2.0958) | Acc: (21.91%) (1711/7808)\n","Epoch: 0 | Batch_idx: 70 |  Loss: (2.0642) | Acc: (23.26%) (2114/9088)\n","Epoch: 0 | Batch_idx: 80 |  Loss: (2.0345) | Acc: (24.29%) (2518/10368)\n","Epoch: 0 | Batch_idx: 90 |  Loss: (2.0079) | Acc: (25.15%) (2929/11648)\n","Epoch: 0 | Batch_idx: 100 |  Loss: (1.9834) | Acc: (26.08%) (3372/12928)\n","Epoch: 0 | Batch_idx: 110 |  Loss: (1.9632) | Acc: (26.72%) (3796/14208)\n","Epoch: 0 | Batch_idx: 120 |  Loss: (1.9460) | Acc: (27.35%) (4236/15488)\n","Epoch: 0 | Batch_idx: 130 |  Loss: (1.9271) | Acc: (28.18%) (4726/16768)\n","Epoch: 0 | Batch_idx: 140 |  Loss: (1.9129) | Acc: (28.73%) (5185/18048)\n","Epoch: 0 | Batch_idx: 150 |  Loss: (1.8973) | Acc: (29.34%) (5670/19328)\n","Epoch: 0 | Batch_idx: 160 |  Loss: (1.8829) | Acc: (29.91%) (6163/20608)\n","Epoch: 0 | Batch_idx: 170 |  Loss: (1.8700) | Acc: (30.39%) (6652/21888)\n","Epoch: 0 | Batch_idx: 180 |  Loss: (1.8557) | Acc: (30.96%) (7172/23168)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-cabad11b9ca0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_decay_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_decay_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     save_checkpoint(default_directory, {\n\u001b[1;32m    298\u001b[0m         \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-cabad11b9ca0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[""],"metadata":{"id":"rIDLBNJxXucT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"5tBMSV0SWCFp"},"execution_count":null,"outputs":[]}]}